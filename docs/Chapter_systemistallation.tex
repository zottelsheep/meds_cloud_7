\chapter{System and environment installation}

This chapter will cover the installation process of a working Kubernetes cluster with one Master-Node and 2 Worker-Nodes with a \texttt{flannel-overlay network} as well as loadbalancing using \texttt{metallb} and ingress-support using \texttt{nginx-ingress} on a Ubuntu system.\\\\The guide is not meant to install a production-grade Kubernetes cluster but will try to "simulate" a professional infrastructure environment.

\section{Prerequisites}
 \begin{itemize}
     \item To perform the server setup and application deployment, access to the server(s), either physical or via a remote connection, is required.
     \item The servers must have an active internet connection, since the installation-process frequently needs to download different dependencies.
 \end{itemize}
\section{Server setup}
Each node must be deployed on its own system, whether virtual or physical. This guide will use virtualization to set up each node on a single server. As a backend we use \texttt{libvirt} + \texttt{qemu with kvm} to setup our virtual machines. For easier configuration of libvirt we use a tool called \texttt{kcli}. Ubuntu is used as the distribution of choice, but other Linux distributions can be used as well.\\\\\noindent The overall steps are as follows:
\begin{itemize}
	\item Install prerequisites to host system
	\item Create a virtual network, so that each VM can communicate to each other
	\item Setup DNS for easier VM identification
	\item Download the image for use inside the VMs
	\item Provision the VMs
\end{itemize}

\noindent Kcli has a feature called \texttt{plans} to construct a virtualized environment. These plans are simply yaml-files that define different resources that together form the virtualized environment.
We'll use this feature to create a reproducible environment and save the configuration as code inside git. This plan will create the virtual network, download the image and create the three VMs. We'll go over the different sections in detail later in this document:
\lstinputlisting[caption=kcli plan]{./kube-install/vms.yaml}
This plan can be executed with the following command:
\begin{lstlisting}[language=bash]
sudo kcli create plan <plan> <name>
\end{lstlisting}

\subsection{Install prerequisites to host system}
Libvirt and qemu can be installed using the system package manager.
\begin{lstlisting}[language=bash] 
sudo apt update
sudo apt install libvirt-qemu-system libvirt-dev
\end{lstlisting}
Kcli on the other hand is a python-based program and currently not available as a Ubuntu package, so we have to install python3 manually and install kcli using pip.
\begin{lstlisting}[language=bash] 
sudo apt install python3 python3-pip python3-venv python3-dev
sudo pip install --system kcli
\end{lstlisting}

\subsection{Creating a key-pair for ssh-access to virtual-machines}
Kcli can automatically register a public key, so that one can ssh directly into freshly baked VM, or for usage with tools such as Ansible.
We'll create a fresh pair of keys and add them to our ssh-agent for maximum comfort:
\begin{lstlisting}[language=bash,caption=Create SSH-Keypair for Kcli] 
ssh-keygen -t ed25519 -C "Kcli" -f ~/.kcli/ssh-key.key
ssh-add ~/.kcli/ssh-key.key
\end{lstlisting}
From now on we can access our newly created VMs with \texttt{kcli ssh <name of VM>}.
\subsection{Create virtual-network}
In a virtual environment there are so called \texttt{Virtual network adapters} that are used to establish network-connectivity to and between other machines. These virtual network adapters are often connected to a \texttt{virtual switch} operating in a multitude of different modes, for example:
\begin{itemize}
	\item \texttt{NAT}: Network Address Translation is most commonly used for simple outward network connectivity but no inward connectivity. In this configuration the outside network cannot talk directly to the VMs, although there is the possibility of Port-Forwarding to enable Guest communication.
	\item \texttt{Bridge-Mode}: In this mode the VMs get directly connected to the outside network similar to a physical machine. This way all communication is managed through the original networking-stack and outward as well as inward traffic is possible.
\end{itemize}
The available modes and configurations are managed by the virtualization-platform and underlying OS.
By default there is neither a connection to the host OS nor to other VMs set up on the system.\\\\In out scenario we want to emulate a real network-stack with basic routing and switching, but do not want to expose this network to the outside. For this we will use a nat-network, since this fits the bill quite nicely.\\\\\textit{Note:} In a production environment it is more likely to use a bridge-network and connect directly to the existing network-stack. This way the network is not limited to a single host-machine.\\\\To create a NAT-network using kcli we specify following in our plan:
\lstinputlisting[
  caption=Plan: Nat-Network,
	firstline=10,
	lastline=13
]{./kube-install/vms.yaml}
This creates a simple nat-network with the appropriate netmask and a domain-name, which is used by the internal dns-server (dnsmasq).
\subsection{Getting prerequisites}
In order to run a VM, we need two things: An image and a place to put our VM files. 
We first create a new \texttt{pool}. This pool is where all VM-Files and downloaded images are kept.\footnote{You may have to change the path to a new absolute path, since relative paths are a bit buggy in the current kcli version}
\lstinputlisting[
  caption=Plan: Pool Definition,
	firstline=1,
	lastline=3
]{./kube-install/vms.yaml}
Then we need to download a fresh Ubuntu-image. Kcli uses a tool called cloudinit to automatically configure images with for example ssh-keys. This tool is embedded onto the image itself, that is why we need special cloudimages. Canonical, the company behind Ubuntu and creator of cloudinit, has a large repository of cloudimages where you can download a variety of different cloud-images for different linux-distributions. This resource downloads the image into our pool and we can reference it by using the resource-name:
\lstinputlisting[
  caption=Plan: Image,
	firstline=5,
	lastline=8
]{./kube-install/vms.yaml}
\subsection{Provision the VMs}
Now we can provision our VMs. We give some basic info about RAM and CPU, specify a disk\footnote{Sadly we can only specify a fixed sized disk using this version of kcli} and select our previously downloaded image.
\lstinputlisting[
  caption=Plan: Example configuration for Node-VM,
	firstline=15,
	lastline=27
]{./kube-install/vms.yaml}
The \texttt{reservedns}, \texttt{reservehosts} and \texttt{reserveip} options are for reserving a dns-name with the resource-name + domain-name (\textit{master0.k8s-thab.local}), storing this name to the local hosts-file for name-resolution on the host-system\footnote{since the dns-server is only available within the virtual-network}, and reserving a static ip-adress for the VM, respectively. 

\section{Setup Nodes}
As we have learned, for Kubernetes each node must run a kubelet and a container-runtime. For our container-runtime we'll use \texttt{CRI-O}, a lightweight cri-compliant runtime, which is made specifically for Kubernetes.
To bootstrap our actual kubelet we'll use a tool called \texttt{kubeadm} which is made to build up and administrate the individual infrastructure-components of a Kubernetes-Cluster.

\subsection{Install CRI-O}

In order to install CRI-O, we'll first have to add the appropriate repositories and keyrings from the CRI-O website.

\begin{lstlisting}[language=bash,caption=Add CRI-O repositories]
# Set Version
export OS=xUbuntu_22.04
export VERSION=1.26

# Download keyrings
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/libcontainers-archive-keyring.gpg
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/libcontainers-crio-archive-keyring.gpg

# Download repository sources
echo "deb [signed-by=/etc/apt/keyrings/libcontainers-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
echo "deb [signed-by=/etc/apt/keyrings/libcontainers-crio-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list

# Update respositories and install packages using apt $
sudo apt update
sudo apt install cri-o crio-o-runc 
\end{lstlisting}

\begin{lstlisting}[language=bash] 
sudo apt update
sudo apt install cri-o crio-o-runc 
\end{lstlisting}

\subsection{Install kubeadm \& kubelet}
Same as with CRI-O, we have to get the repositories and corresponding keys before we can use our package-manager to install the needed packages:

\begin{lstlisting}[language=bash] 
# Get respositories and keyrings
sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Update respositories and install packages using apt
sudo apt update
sudo apt install kubeadm kubelet
\end{lstlisting}
We can then install the packages:
\begin{lstlisting}[language=bash] 
sudo apt update
sudo apt install kubeadm kubelet
\end{lstlisting}

\subsection{Configure System for Kubernetes}

We have to enable some kernelmodules for CRI-O to function properly with Kubernetes on Ubuntu and enable ip-forwarding. In order for these changes to persist a reboot-cycle we save them into \textit{/etc/modules-load.d/crio.conf} and \textit{/etc/sysctl.d/99-kubernetes.conf} respecivly.

\begin{lstlisting}[language=bash,caption=Enable kernelmodules] 
sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/modules-load.d/crio.conf
overlay
br_netfilter
EOF
\end{lstlisting}

\begin{lstlisting}[language=bash,caption=Enable IP-Forwarding] 
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system
\end{lstlisting}

\subsection{Start CRI-O and kubelet}
As a last step we need to enable and start the services using \texttt{systemctl}:

\begin{lstlisting}[language=bash,caption=Starting CRI-O and kubelet services] 
# Enable and start CRI-O
sudo systemctl enable crio.service
sudo systemclt start crio.service
sudo systemctl status crio.serivce

# Enable and start kubelet
sudo systemctl enable kubelet.service
sudo systemclt start kubelet.service
sudo systemctl status kubelet.serivce
\end{lstlisting}
CRI-O should now be started and working. The kubelet will also start, but constantly be crashing, since it is waiting for further configuration from kubeadm.

\section{Bootstrapping Kubernetes}
\subsection{Setup Master-Node}
To initalize the cluster we need to create our first Master-Node or control-plane.\\\\The command for that is really simple:
\begin{lstlisting}[language=bash,caption=Initialize Controle-Plane] 
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint master0.k8s-thab.local
\end{lstlisting}
The \texttt{pod-network-cidr} is used to set the netmask for the internal cluster-network. We need to set this explicitly to this netmask since our network-plugin \texttt{flannel} requires this as a default.
We also set the \texttt{control-plane-endpoint} to point to our dns-entry that corresponds with the Master-Node-VM.
Kubeadm will perform serveral checks to make sure the system is compatible and afterwards bootstrap the kublet as control-plane-node.
After kubeadm is finished it will print the join-command for bootstrapping worker-nodes and joining the cluster as well as instructions on how to connect to the control-plane via \texttt{kubectl}.\\\\\textit{Note:} For a production-ready cluster it is advised to use at least three control-plane nodes. Originally we had planned to also do the same and use an additional VM to act as an loadbalancer for the control-planes available under \texttt{api.k8s-thab.local} but since we had limited resources we chose to leave it at one node without further loadbalancing.

\subsection{Setup Worker-Node}
The process to bootstrap a Worker-Node and get it to join a cluster is rather simple. We just need to execute the command, that the control-plane gave us previously. If the command got somehow lost, we can simply recreate a new token on the master-node with:
\begin{lstlisting}[language=bash,caption=Get new Join-Token for Worker-Node] 
sudo kubeadm token create --print-join-command
\end{lstlisting}
\begin{lstlisting}[language=bash,caption=Example Join-Command] 
sudo kubeadm join master0.k8s-thab.local:6443 --token <token> --discovery-token-ca-cert-hash <sha256-hash>
\end{lstlisting}
Mind we also need to execute this command with \texttt{sudo} to get elevated privileges. This step can take a while, especially when adding a node to a completely configured cluster, since it needs to install networking and other resources, from various plugins.\\\\We can check the status using \texttt{kubectl get nodes}.

\section{Setup Cluster-Networking and Loadbalancing}
\subsection{Cluster-Networking}
After bootstrapping the master and worker nodes, we still need to deploy our cluster-networking plugin. In our case we chose the \texttt{flannel-overlay} plugin.
To install we only have to apply one configuration-yaml using kubectl:
\begin{lstlisting}[language=bash,caption=Deploy Flannel-Networking] 
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
\end{lstlisting}
This will grab the \textit{v0.20.2} Version of Flannel and install it on our cluster.
After waiting about 5min, the installation should be finished and Initialized. To check we can again use kubectl to show us the pods. If coredns is running we know that it worked.
\subsection{Loadbalancing}
In order for loadbalancing to work in Kubernetes, we have to supply some way for Kubernetes to register an external-ip together with the corresponding cluster-ips. Since we have no real loadbalancing hardware, we'll use a program called \texttt{MetalLB}.
MetalLB basically uses an existing node to act as a loadbalancer by assigning it multiple ips and setting up special routes for that machine.\\\\It is also possible to use it in \texttt{bgb-mode} which utilizes the bgp-protocol to advertise routes within the network. This approach would allow for true loadbalacing, but is out-of-scope of this project, since it would require setting up a working internal bgp-infrastructure.\\\\To deploy MetalLB we'll use following configuration-yaml:
\begin{lstlisting}[language=bash,caption=Deploy MetalLB Loadbalacing] 
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml
\end{lstlisting}
\noindent The final step is to tell metallb which ips it can assign as an external-ip. For this we specify an ip-range within our virtual-network:
\begin{lstlisting}[language=bash,caption=Set assinable IP-Ranges for external IPs] 
cat <<EOF | kubectl apply -f -
---
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
 name: default
 namespace: metallb-system
spec:
 addresses:
 - 192.168.111.20-192.168.111.120
 autoAssign: true
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
 name: default
 namespace: metallb-system
spec:
 ipAddressPools:
 - default
EOF
\end{lstlisting}

\subsection{Setup Ingress-Support}
The last step we have to do is installing an ingress-controller in order for ingress-services to work. We have opted to use the \texttt{nginx-ingress} controller.\\\\To deploy the ingress-controller we'll use following configuration-yaml:
\begin{lstlisting}[language=bash,caption=Deploy nginx-ingress as ingress-controller] 
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml
\end{lstlisting}
\vspace{2cm}
\begin{center}
    \Large{... and that's it!}    
\end{center}

